{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "adeb60ba",
   "metadata": {},
   "source": [
    "\n",
    "# Credit Risk Prediction using German Credit Dataset\n",
    "\n",
    "This notebook develops a machine learning model to classify loan applicants into **good** or **bad credit risk** categories using the German Credit dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1ba67f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from xgboost import XGBClassifier\n",
    "import shap\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d87d60d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Load dataset\n",
    "df = pd.read_csv(\"german_credit_data.csv\")\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a416f4e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df.info()\n",
    "df.describe(include='all')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d01359d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Check and handle missing values\n",
    "df.isnull().sum()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2adba1f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Visualizing target distribution and correlations\n",
    "sns.countplot(data=df, x='Risk')\n",
    "plt.title(\"Credit Risk Distribution\")\n",
    "plt.show()\n",
    "\n",
    "# Correlation heatmap for numeric features\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.heatmap(df.select_dtypes(include=[np.number]).corr(), annot=True, cmap='coolwarm')\n",
    "plt.title(\"Feature Correlation Heatmap\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ba9ba3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Encode categorical variables\n",
    "categorical_cols = df.select_dtypes(include=['object']).columns\n",
    "for col in categorical_cols:\n",
    "    if col != 'Risk':\n",
    "        df[col] = LabelEncoder().fit_transform(df[col])\n",
    "\n",
    "# Encode target\n",
    "df['Risk'] = df['Risk'].map({'good': 1, 'bad': 0})\n",
    "\n",
    "# Feature and target separation\n",
    "X = df.drop('Risk', axis=1)\n",
    "y = df['Risk']\n",
    "\n",
    "# Standardize numerical features\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "051e802c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Train multiple models\n",
    "models = {\n",
    "    \"Random Forest\": RandomForestClassifier(),\n",
    "    \"Logistic Regression\": LogisticRegression(),\n",
    "    \"SVM\": SVC(),\n",
    "    \"XGBoost\": XGBClassifier(use_label_encoder=False, eval_metric='logloss')\n",
    "}\n",
    "\n",
    "for name, model in models.items():\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    print(f\"Model: {name}\")\n",
    "    print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "    print(\"Precision:\", precision_score(y_test, y_pred))\n",
    "    print(\"Recall:\", recall_score(y_test, y_pred))\n",
    "    print(\"F1 Score:\", f1_score(y_test, y_pred))\n",
    "    print(confusion_matrix(y_test, y_pred))\n",
    "    print(\"-\" * 40)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82f516ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Interpret model using SHAP (example with Random Forest)\n",
    "explainer = shap.TreeExplainer(models[\"Random Forest\"])\n",
    "shap_values = explainer.shap_values(X_train)\n",
    "shap.summary_plot(shap_values[1], X_train, feature_names=X.columns)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "604630a2",
   "metadata": {},
   "source": [
    "\n",
    "## Key Insights and Recommendations\n",
    "\n",
    "- **Top influential features** identified by SHAP can guide credit policy reviews.\n",
    "- **Feature scaling and encoding** significantly impacted model accuracy and interpretability.\n",
    "- **Model performance** suggests Random Forest and XGBoost performed best on this dataset.\n",
    "- Consider **enhancing the dataset** with more recent or real-time financial indicators.\n",
    "- Automate preprocessing and scoring for real-time credit risk assessment.\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
